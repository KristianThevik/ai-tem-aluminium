{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8dde01f-50ea-47aa-b181-5bb8c769b80f",
   "metadata": {},
   "source": [
    "# Automatic segmentation of precipitate statistics using U-Net architecture\n",
    "\n",
    "This code utilizes a U-Net deep learning architecture to automatically extract precipitate statistics from transmission electron microscopy (TEM) images. \n",
    "The code is developed as part of a master's thesis in applied physics, the code segments precipitates within the images, enabling the automatic measurement of precipitate length and cross-sections. \n",
    "By automating this process, it significantly accelerates the analysis of precipitate distributions, aiding in materials research and development.\n",
    "\n",
    "## Author:\n",
    "\n",
    "**Espen J. Gregory** - Developed for Master thesis in Physics 2024\n",
    "\n",
    "## Note:\n",
    "- It is recommended to have a GPU and the CUDA-version of Pytorch installed (However it is not required).\n",
    "- Make sure model files (.pth) are placed in the same folder as the notebook\n",
    "- Data can be loaded in two ways, either by directly uploading the .DM3 file, or converting the .DM3 to an image (.jpeg/.png) and manually selecting the calibration unit *nm_per_px*.\n",
    "- U-Net documentation: https://arxiv.org/abs/1505.04597\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b381c-853f-49aa-b4f5-fc954f654336",
   "metadata": {},
   "source": [
    "### Imports and PyTorch initializationk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4dbfa4-f712-4bf3-9f62-8cf306b34b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\krist\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install pandas\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "#import _dm3_lib as dm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from tkinter import filedialog\n",
    "#from u_net_pytorch import UNet\n",
    "from skimage import measure, color, io\n",
    "from skimage.segmentation import clear_border\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"PyTorch Initialization\"\"\"\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = True\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device type: %s\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee5aae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\Documents\\masterRepo\\src\\testMaster\\DatasetEvaluator.py:276: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(model, map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unet Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 35.46 nm, STDev: 25.71 nm, Number counted: 148, STDev of mean: 2.11 nm, Number density: 0.0002863nm^-2\n",
      "663 [20.0079999924, 47.0090000153, 16.005, 21.0089999962, 13.0019999981, 15.0080000019, 22.0020000076, 20.0089999962, 9.00699999809, 18.0070000076, 9.0, 30.0039999962, 17.0, 13.0019999981, 26.0079999924, 17.0029999924, 17.0029999924, 25.0, 23.0010000038, 16.0039999962, 15.0010000038, 23.0, 20.0020000076, 17.005, 17.0020000076, 21.0029999924, 8.00699999809, 57.0079999924, 49.009999847, 13.0069999981, 26.0079999924, 15.0030000019, 11.0010000038, 16.005, 22.0010000038, 24.0029999924, 30.0060000038, 26.0010000038, 23.0029999924, 20.005, 11.005, 33.0029999924, 45.0040000153, 15.0010000038, 10.005, 24.0029999924, 23.0020000076, 10.0019999981, 12.0080000019, 27.0089999962, 9.00800000191, 24.0010000038, 27.0029999924, 21.0060000038, 14.0089999962, 32.005, 16.0079999924, 11.0069999981, 15.005, 14.0030000019, 20.0060000038, 43.0079999924, 32.0, 19.0029999924, 19.005, 16.0010000038, 30.0070000076, 35.0090000153, 21.0010000038, 27.0020000076, 68.0069999695, 11.0019999981, 15.0030000019, 35.0020000076, 31.005, 24.0029999924, 25.005, 27.0020000076, 14.0019999981, 17.0079999924, 41.0, 15.0060000038, 16.0070000076, 15.0030000019, 16.0, 37.0040000153, 17.0039999962, 20.0089999962, 17.0010000038, 17.0020000076, 11.0080000019, 23.0039999962, 14.0069999981, 23.0039999962, 29.0070000076, 9.00100000381, 18.0020000076, 11.0080000019, 20.005, 17.0079999924, 17.0010000038, 35.0, 15.0069999981, 46.0079999924, 14.0069999981, 14.0069999981, 13.0019999981, 8.005, 27.0039999962, 12.0080000019, 18.0, 12.0039999962, 10.0030000019, 19.0089999962, 19.0020000076, 37.0, 18.0, 28.0089999962, 24.0089999962, 9.00399999619, 13.005, 9.00399999619, 17.0060000038, 27.0079999924, 22.0089999962, 24.0070000076, 46.0020000076, 11.0080000019, 20.0089999962, 19.0079999924, 24.0029999924, 15.0060000038, 18.0039999962, 18.0039999962, 26.005, 24.0, 11.0080000019, 13.0019999981, 13.0019999981, 25.0020000076, 17.0070000076, 18.005, 15.0069999981, 9.00399999619, 9.00800000191, 12.0069999981, 18.0, 27.0079999924, 36.0, 24.0089999962, 20.005, 17.0, 23.0089999962, 26.0039999962, 15.0060000038, 10.0069999981, 11.0080000019, 17.0029999924, 17.0029999924, 11.0039999962, 36.0, 10.0069999981, 11.0010000038, 21.0039999962, 24.0020000076, 20.005, 10.0039999962, 13.005, 29.0079999924, 10.0039999962, 20.0089999962, 20.0039999962, 19.0039999962, 19.0070000076, 32.005, 31.0029999924, 31.0039999962, 20.0029999924, 55.0079999924, 25.0079999924, 36.0029999924, 121.003000031, 33.0020000076, 19.0089999962, 28.0060000038, 14.0, 11.0069999981, 35.005, 19.0070000076, 22.0029999924, 9.00399999619, 9.00399999619, 9.005, 10.0080000019, 19.0010000038, 21.0029999924, 11.0080000019, 25.0010000038, 15.0060000038, 12.0019999981, 15.0060000038, 16.0070000076, 16.005, 22.0039999962, 22.0, 34.0079999924, 20.0, 66.0019999695, 14.0030000019, 22.0, 11.005, 29.0020000076, 10.0080000019, 14.0, 38.0059999847, 12.0060000038, 25.0020000076, 15.0089999962, 27.0, 18.0079999924, 82.0040000153, 11.005, 18.0079999924, 18.005, 13.0060000038, 16.0039999962, 18.0020000076, 25.0020000076, 44.0020000076, 12.0069999981, 14.0030000019, 17.0020000076, 11.005, 23.005, 27.0089999962, 34.009999847, 16.0020000076, 32.0040000153, 15.0, 24.0010000038, 18.005, 18.005, 13.0039999962, 38.0, 49.0029999924, 51.0029999924, 18.0089999962, 15.0080000019, 16.0010000038, 15.0030000019, 90.009999847, 23.0, 32.0079999924, 21.0039999962, 42.0040000153, 21.0089999962, 12.0019999981, 16.005, 8.00399999619, 11.0010000038, 24.0010000038, 28.0060000038, 67.0019999695, 12.0089999962, 18.0010000038, 22.0, 19.005, 16.0020000076, 13.0030000019, 28.0070000076, 21.0010000038, 10.0010000038, 10.0010000038, 18.0, 8.00699999809, 18.0039999962, 18.0089999962, 30.0060000038, 22.0070000076, 18.0020000076, 19.0010000038, 26.0070000076, 33.0040000153, 11.005, 21.0089999962, 36.0090000153, 63.0, 26.0020000076, 21.0089999962, 18.0020000076, 22.0070000076, 21.0070000076, 11.0039999962, 26.0020000076, 12.0010000038, 21.0060000038, 10.0010000038, 25.0070000076, 16.0060000038, 34.009999847, 21.0029999924, 20.0020000076, 21.0029999924, 42.0079999924, 73.0059999847, 18.0079999924, 16.0010000038, 41.0079999924, 39.0070000076, 13.005, 32.0040000153, 9.00399999619, 16.0010000038, 9.00800000191, 9.00100000381, 11.0019999981, 13.0, 21.0089999962, 18.0020000076, 8.0, 8.00100000381, 17.0020000076, 21.0060000038, 25.0020000076, 23.0010000038, 28.0020000076, 17.0020000076, 13.005, 22.0070000076, 18.0060000038, 18.0060000038, 19.0089999962, 18.0020000076, 21.0089999962, 29.0079999924, 11.0039999962, 14.0030000019, 14.0089999962, 19.0039999962, 24.0060000038, 19.0079999924, 44.005, 17.0010000038, 14.0089999962, 23.0039999962, 51.0079999924, 41.0070000076, 27.0070000076, 26.0039999962, 23.0039999962, 17.0020000076, 28.0020000076, 27.005, 25.0079999924, 15.0039999962, 15.0019999981, 16.0010000038, 17.0060000038, 13.0, 44.0079999924, 18.0089999962, 19.0060000038, 24.0010000038, 13.0060000038, 18.0060000038, 54.009999847, 16.0070000076, 20.0070000076, 14.0060000038, 11.0019999981, 16.0010000038, 29.0039999962, 17.0070000076, 29.0089999962, 23.0029999924, 15.0069999981, 23.0070000076, 21.0079999924, 24.0, 53.0020000076, 13.0, 16.0070000076, 35.0090000153, 13.005, 11.005, 13.0019999981, 18.0039999962, 72.0, 24.0029999924, 10.0010000038, 16.0079999924, 11.0089999962, 16.0060000038, 11.0039999962, 27.0010000038, 17.0070000076, 16.0070000076, 20.0039999962, 10.005, 39.0079999924, 27.005, 37.0, 10.0010000038, 27.0020000076, 21.0089999962, 27.0089999962, 9.00399999619, 24.0020000076, 24.005, 58.0090000153, 15.0069999981, 11.0019999981, 19.0020000076, 14.0060000038, 18.0089999962, 22.0070000076, 19.005, 17.0079999924, 18.005, 30.0010000038, 9.0, 12.0080000019, 71.0080000305, 26.005, 16.0039999962, 31.005, 24.0010000038, 20.0010000038, 18.0010000038, 15.0039999962, 16.0, 23.0060000038, 21.0060000038, 18.0079999924, 35.0040000153, 24.0, 15.0039999962, 12.0089999962, 16.0070000076, 18.0079999924, 14.0069999981, 33.005, 16.0079999924, 21.0, 72.0069999695, 12.0039999962, 23.005, 19.0060000038, 33.0, 18.0010000038, 23.0, 28.0089999962, 26.0029999924, 16.0, 19.0079999924, 19.0060000038, 19.0010000038, 19.0060000038, 57.0070000076, 28.0079999924, 21.0089999962, 58.005, 13.0030000019, 13.0060000038, 13.0060000038, 12.0089999962, 12.005, 19.005, 17.0, 8.00399999619, 15.0069999981, 11.005, 12.0, 33.0, 13.005, 21.0, 11.0019999981, 25.0010000038, 28.0010000038, 29.0060000038, 23.0070000076, 25.0, 43.0059999847, 14.0030000019, 17.0010000038, 35.0090000153, 14.0019999981, 19.0079999924, 23.0070000076, 14.0060000038, 26.0079999924, 12.0019999981, 36.0020000076, 13.0019999981, 21.005, 9.00199999809, 22.0029999924, 21.0, 14.0060000038, 14.0060000038, 22.0, 11.005, 13.0, 14.0030000019, 17.0020000076, 16.0010000038, 28.0020000076, 13.0, 20.0060000038, 24.0, 16.0039999962, 50.009999847, 29.0029999924, 14.0030000019, 16.0070000076, 20.0029999924, 27.0060000038, 13.0, 21.0060000038, 25.0020000076, 17.005, 18.0020000076, 24.0070000076, 14.0089999962, 8.0, 9.00699999809, 17.0010000038, 16.0039999962, 11.005, 32.0040000153, 30.0060000038, 21.0060000038, 22.0020000076, 14.0060000038, 38.0, 14.0069999981, 29.0060000038, 24.0, 18.0089999962, 7.00699999809, 23.0010000038, 9.00399999619, 46.0090000153, 21.0089999962, 18.0029999924, 24.0, 18.0060000038, 16.0010000038, 21.0010000038, 13.0019999981, 22.0020000076, 19.005, 17.0079999924, 140.003000031, 25.005, 28.0029999924, 30.0010000038, 15.0010000038, 17.0039999962, 16.0010000038, 6.00900000095, 15.0039999962, 15.0039999962, 15.0039999962, 20.005, 10.005, 13.0089999962, 17.0, 15.0030000019, 14.0030000019, 18.0060000038, 11.0019999981, 20.0079999924, 16.0079999924, 15.0060000038, 11.0010000038, 11.0019999981, 10.005, 18.0089999962, 20.0029999924, 20.005, 18.0089999962, 22.0029999924, 16.0039999962, 26.005, 16.0029999924, 15.0069999981, 31.0039999962, 17.0020000076, 18.0070000076, 18.005, 20.0060000038, 12.005, 11.005, 11.0069999981, 13.0019999981, 22.0, 20.0010000038, 20.005, 30.0010000038, 16.0029999924, 12.0, 12.0010000038, 23.0, 20.0010000038, 30.0029999924, 18.005, 14.0080000019, 13.0060000038, 13.0089999962, 28.0020000076, 21.0079999924, 27.0020000076, 25.0010000038, 13.0089999962, 11.0010000038, 9.00199999809, 7.00300000191, 17.0039999962, 41.0079999924, 13.0060000038, 24.0070000076, 14.0030000019, 39.009999847, 18.0020000076, 25.0070000076, 13.0089999962, 23.0, 26.0010000038, 13.0060000038, 22.0089999962, 21.0039999962, 20.0010000038, 14.0, 24.0020000076, 33.0020000076, 22.0, 21.0070000076, 16.0089999962, 6.0099999905, 16.005, 22.0, 45.009999847, 65.005, 18.005, 16.0079999924, 13.0080000019, 19.0029999924, 27.0079999924, 11.0019999981, 12.0089999962, 28.0029999924, 16.0070000076, 16.0, 30.0070000076, 39.009999847, 40.0, 18.0079999924, 16.0, 15.0089999962, 31.0029999924, 18.0079999924, 32.0070000076]\n",
      "Mean:  21.57158974184698 STD:  12.68897247509504\n"
     ]
    }
   ],
   "source": [
    "# Using DataSetEvaluator\n",
    "from testMaster.DatasetEvaluator import UNETEvaluator\n",
    "\n",
    "# If files in the dataset are .dm3 files, nm_per_px will be extracted automatically and replace the \n",
    "# existing nm_per_px value in the list. It works to have some files as dm3 and some as .jpg/.png.\n",
    "# Always give nm_per_px as a list with lenght equal to number of files to be evaluated. First value corresponds\n",
    "# to nm_per_px value for the first image and so on. For dm3 files an arbitrary value can be given, it will be replaced\n",
    "# either way.\n",
    "\n",
    "\n",
    "nm_per_px = [0.069661] * 1 #Cross\n",
    "#nm_per_px = [0.16685] * 8 #Length\n",
    "\n",
    "# Example usage:\n",
    "this_dir = Path.cwd()\n",
    "dataset_path = this_dir.parent / \"data\" / \"test_length_dm3\"\n",
    "model_path = this_dir.parent / \"data\" / \"models\" / \"length_unet.pth\"\n",
    "\n",
    "\n",
    "unet_evaluator = UNETEvaluator(\n",
    "    dataset_dir = dataset_path,\n",
    "    model = model_path,\n",
    "    nm_per_px = nm_per_px,\n",
    "    cross = False,\n",
    "    device = 'cpu'\n",
    ")\n",
    "\n",
    "unet_evaluator.statistics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3efb40-9249-4d24-ad42-653b0f9b3505",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a259d3d1-8cef-4092-bbaa-e1732194c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_img(arr, d) -> list:\n",
    "    \n",
    "    \"\"\"\n",
    "    Tile the image into equal parts of size (d x d) pixels\n",
    "\n",
    "    img: PIL Image\n",
    "    d  : Side length of square tile\n",
    "\n",
    "    Return: List of PIL images\n",
    "    \"\"\"\n",
    "    img = Image.fromarray(arr)\n",
    "    w, h = img.size\n",
    "    grid = product(range(0, h-h%d, d), range(0, w-w%d, d))\n",
    "    img_list = []\n",
    "    for i, j in grid:\n",
    "        box = (j, i, j+d, i+d)\n",
    "        img_list.append(img.crop(box))\n",
    "    return img_list\n",
    "\n",
    "\n",
    "def DM_2_array(img) -> np.array:\n",
    "    \"\"\"\n",
    "    Convert Digital Micrograph file to numpy array\n",
    "\n",
    "    img: An instance of the DM3 class from _dm3_lib.py\n",
    "\n",
    "    returns a numpy array of the grayscale image\n",
    "    \"\"\"\n",
    "    nm_per_px = img.pxsize[0]\n",
    "    cons = img.contrastlimits\n",
    "    im   = img.imagedata\n",
    "    im[im>cons[1]] = cons[1]\n",
    "    im[im<cons[0]] = cons[0]\n",
    "    im =  ((im-cons[0])/(cons[1]-cons[0]))*255  #0 to 1\n",
    "    return im.astype(np.uint8), nm_per_px\n",
    "\n",
    "def load_data() -> list:\n",
    "    \n",
    "    \"\"\"\n",
    "    Opens dialogbox that allows selection of files\n",
    "    Returns file/files\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "    files = filedialog.askopenfilenames(parent=root, title='Choose a file')\n",
    "    return files\n",
    "\n",
    "\n",
    "class Prediction():\n",
    "    def __init__(self,cross):\n",
    "        self.cross     = cross\n",
    "        self.size      = 1024\n",
    "        self.tile_size = 512\n",
    "        if self.cross:\n",
    "            self.PATH = r'C:\\Users\\krist\\Documents\\masterRepo\\data\\models\\cross_unet.pth'\n",
    "        else:\n",
    "            self.PATH = r\".\\models\\length_unet.pth\"\n",
    "\n",
    "        self.checkpoint = torch.load(self.PATH, map_location=torch.device('cpu'))\n",
    "        self.model      = UNet(in_channels = 1, n_classes = 2, depth = 3, wf = 6, padding = True)\n",
    "        self.model.load_state_dict(self.checkpoint['model_state_dict'])\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        print('Model Loaded')\n",
    "    \n",
    "    def to_tensor(self, file) -> list:\n",
    "        \"\"\"\n",
    "        \n",
    "        Opens the image in grayscale, resizes it (if applicable), and converts it to a pytorch tensor\n",
    "    \n",
    "        file   (str)    : Path to file\n",
    "        Returns tensor or list of tensors\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            if file.endswith('.dm3'):\n",
    "                \n",
    "                image, nm_per_px = DM_2_array(dm.DM3(file))\n",
    "                if len(self.images) == 0:\n",
    "                    self.nm_per_px = nm_per_px\n",
    "            else:\n",
    "                image = np.array(Image.open(file).convert('L'))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Something went wrong when loading the image.\")\n",
    "            pass\n",
    "            \n",
    "        image   = cv2.resize(image, dsize = (self.size,self.size))\n",
    "        tensors = []\n",
    "        images  = tile_img(np.array(image), self.tile_size)\n",
    "\n",
    "        for i in images:\n",
    "            \n",
    "            im = np.expand_dims(i, axis=0)\n",
    "            \n",
    "            ## Can add filter if images are very noisy (Median recommended, gaussian makes the images too blurry)\n",
    "            # image = nd.median_filter(image, size=3) \n",
    "            im = 2*(im/np.max(im)) - 1\n",
    "            im = torch.tensor(im, dtype = torch.float32)\n",
    "            tensors.append(im)\n",
    "        return np.array(image), tensors\n",
    "\n",
    "\n",
    "    \n",
    "    def watershed(self,img, plot = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Performs the watershed algorithm on the prediction img\n",
    "    \n",
    "        img  : PIL.Image (semantic segmentation prediction map)\n",
    "        plot : bool (True if you want to see the watershed processing steps)\n",
    "        \n",
    "        Documentation: https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html\n",
    "        \n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = clear_border(gray)\n",
    "        ret, bin_img = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU) \n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        sure_bg = cv2.dilate(bin_img, kernel, iterations=20) \n",
    "        dist = cv2.distanceTransform(bin_img, cv2.DIST_L2, 5) \n",
    "        \n",
    "       \n",
    "        #foreground area \n",
    "        ret, sure_fg = cv2.threshold(dist, 0.15 * dist.max(), 255, cv2.THRESH_BINARY) \n",
    "        sure_fg = sure_fg.astype(np.uint8)   \n",
    "          \n",
    "        # unknown area \n",
    "        unknown = cv2.subtract(sure_bg, sure_fg) \n",
    "        ret, markers = cv2.connectedComponents(sure_fg) \n",
    "          \n",
    "        # Add one to all labels so that background is not 0, but 1 \n",
    "        markers += 1\n",
    "        markers[unknown == 255] = 0\n",
    "        markers = cv2.watershed(img, markers) \n",
    "        \n",
    "        if plot:\n",
    "            fig, axes = plt.subplots(2,2)\n",
    "            axes[0,0].imshow(gray) \n",
    "            axes[0, 0].set_title('Img') \n",
    "            axes[0,1].imshow(dist) \n",
    "            axes[0, 1].set_title('Distance Transform') \n",
    "              \n",
    "            axes[1,0].imshow(sure_fg) \n",
    "            axes[1, 0].set_title('Sure Foreground') \n",
    "            axes[1,1].imshow(markers) \n",
    "            axes[1, 1].set_title('Markers') \n",
    "    \n",
    "        img2 = color.label2rgb(markers,bg_label = 1,bg_color=(0, 0, 0))\n",
    "        props = measure.regionprops_table(markers, intensity_image=gray, \n",
    "                                      properties=['label',\n",
    "                                                  'area', 'equivalent_diameter',\n",
    "                                                  'mean_intensity', 'solidity'])\n",
    "        \n",
    "        df = pd.DataFrame(props)\n",
    "        area = list(df[(df.mean_intensity > 100) & (df.area > 1.5/self.nm_per_px**2)].area)\n",
    "        return area\n",
    "\n",
    "\n",
    "    \n",
    "    def calc_length(self, img):\n",
    "        \"\"\"\n",
    "        Estimates the length of precipitates\n",
    "        \"\"\"\n",
    "        grey = img[:,:,0]\n",
    "        contours, hierarchy = cv2.findContours(grey, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "        l      = []\n",
    "        for contour in contours:\n",
    "            (center), (width,height), angle = cv2.minAreaRect(contour)\n",
    "            length = np.max([width,height])\n",
    "            l.append([length, angle+(angle<0)*90])\n",
    "        for index, (length, angle) in enumerate(l):\n",
    "            median_angle = np.median([angle for (length, angle) in l if length*self.nm_per_px > 5]) #Find angles of all detections longer than 5nm\n",
    "            error        = 5.0 #degrees\n",
    "            if  (median_angle - error<angle<median_angle + error) and length*self.nm_per_px>3: #If precipitate in correct direction (within error) and longer than 3nm, accept detection\n",
    "                self.lengths.append(length) \n",
    "    def evaluate(self, nm_per_px):\n",
    "        \"\"\"\n",
    "        Evaluation/prediction function\n",
    "\n",
    "        nm_per_px: float (Image calibration AS IF IMAGE IS 2048x2048)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data       = load_data()\n",
    "        self.nm_per_px  = nm_per_px\n",
    "        self.prediction = []\n",
    "        self.images     = []\n",
    "        self.area       = []\n",
    "        self.lengths    = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for img in iter(self.data):\n",
    "            true_img, imgs = self.to_tensor(img)\n",
    "            new_im = Image.new('RGB', (self.size, self.size))\n",
    "            if len(self.images) == 0:\n",
    "                self.nm_per_px *=2 #Original calibration for 2048x2048, but images are resized to 1024x1024\n",
    "            for index, im in enumerate(imgs):\n",
    "                im = im.unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    pred   = self.model(im)\n",
    "                    output = torch.argmax(pred, dim=1)  # Get the index of the channel with the highest probability\n",
    "                    output = output.squeeze(0).cpu().numpy()\n",
    "                    \n",
    "                y_offset =  int(self.tile_size*(index>1))\n",
    "                x_offset =  int(self.tile_size*((index)%2))\n",
    "                out      = Image.fromarray(output.astype('uint8')*255).convert('RGB')\n",
    "                new_im.paste(out, box = (x_offset,y_offset))\n",
    "                \n",
    "            self.images.append(np.array(true_img))\n",
    "            self.prediction.append(np.array(new_im))\n",
    "\n",
    "            if self.cross:\n",
    "                self.area += self.watershed(self.prediction[-1], plot = False)\n",
    "            else:\n",
    "                self.calc_length(self.prediction[-1])\n",
    "                \n",
    "        \n",
    "        total_time = time.time()-start_time\n",
    "        print(f\"Total interference time: {np.round(total_time,2)}s ; Time per Image: {np.round(total_time/len(self.images),3)}s\")\n",
    "\n",
    "        if self.cross:\n",
    "            return np.array(self.area)*self.nm_per_px**2, self.images, self.prediction\n",
    "        else:\n",
    "            return np.array(self.lengths)*self.nm_per_px, self.images, self.prediction        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063cb66-dd29-42f7-b105-d6771acb88ae",
   "metadata": {},
   "source": [
    "## Cross-section\n",
    "\n",
    "Note: nm_per_px (Calibration) should be the calibration for a 2048x2048 image, if the images are .dm3, the manual calibration is not needed. \n",
    "\n",
    "The original images as prediction masks are found in (images, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509356d-d7a5-4e16-9e78-f85c1f074003",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sections = Prediction(cross = True)\n",
    "area, images, prediction = cross_sections.evaluate(nm_per_px = 0.069661)\n",
    "print(len(prediction))\n",
    "\n",
    "print('Average: {0:.2f}nm, STDev: {1:.2f}nm, Number counted: {2:d}'.format(np.mean(area), np.std(area), len(area)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c611f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0de441-4563-40a8-98b5-24f8777cc073",
   "metadata": {},
   "source": [
    "## Length\n",
    "\n",
    "Note: nm_per_px (Calibration) should be the calibration for a 2048x2048 image, if the images are .dm3, the manual calibration is not needed. \n",
    "\n",
    "The original images as prediction masks are found in (images, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb2bdb-38aa-43a3-9892-61cb14fc2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = Prediction(cross = False)\n",
    "length, images, prediction = length.evaluate(nm_per_px = 0.16685)\n",
    "\n",
    "print('Average: {0:.2f}nm, STDev: {1:.2f}nm, Number counted: {2:d}'.format(np.mean(length), np.std(length), len(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88394e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
