{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8dde01f-50ea-47aa-b181-5bb8c769b80f",
   "metadata": {},
   "source": [
    "# Automatic segmentation of precipitate statistics using U-Net architecture\n",
    "\n",
    "This code utilizes a U-Net deep learning architecture to automatically extract precipitate statistics from transmission electron microscopy (TEM) images. \n",
    "The code is developed as part of a master's thesis in applied physics, the code segments precipitates within the images, enabling the automatic measurement of precipitate length and cross-sections. \n",
    "By automating this process, it significantly accelerates the analysis of precipitate distributions, aiding in materials research and development.\n",
    "\n",
    "## Author:\n",
    "\n",
    "**Espen J. Gregory** - Developed for Master thesis in Physics 2024\n",
    "\n",
    "## Note:\n",
    "- It is recommended to have a GPU and the CUDA-version of Pytorch installed (However it is not required).\n",
    "- Make sure model files (.pth) are placed in the same folder as the notebook\n",
    "- Data can be loaded in two ways, either by directly uploading the .DM3 file, or converting the .DM3 to an image (.jpeg/.png) and manually selecting the calibration unit *nm_per_px*.\n",
    "- U-Net documentation: https://arxiv.org/abs/1505.04597\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b381c-853f-49aa-b4f5-fc954f654336",
   "metadata": {},
   "source": [
    "### Imports and PyTorch initializationk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f4dbfa4-f712-4bf3-9f62-8cf306b34b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Requirement already satisfied: pandas in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\krist\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krist\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Device type: cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install pandas\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "import _dm3_lib as dm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from tkinter import filedialog\n",
    "from u_net_pytorch import UNet\n",
    "from skimage import measure, color, io\n",
    "from skimage.segmentation import clear_border\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"PyTorch Initialization\"\"\"\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = True\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device type: %s\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee5aae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\Documents\\masterRepo\\src\\testMaster\\DatasetEvaluator.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.tile_size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unet Model Loaded\n"
     ]
    }
   ],
   "source": [
    "# Using DataSetEvaluator\n",
    "from testMaster.DatasetEvaluator import UNETEvaluator\n",
    "\n",
    "# Example usage:\n",
    "this_dir = Path.cwd()\n",
    "dataset_path = this_dir.parent / \"data\" / \"test_cross\"\n",
    "model_path = this_dir.parent / \"data\" / \"models\" / \"cross_unet.pth\"\n",
    "\n",
    "\n",
    "unet_evaluator = UNETEvaluator(\n",
    "    dataset_dir = dataset_path,\n",
    "    model = model_path,\n",
    "    cross = True,\n",
    "    device = 'cpu'\n",
    ")\n",
    "\n",
    "unet_evaluator.statistics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3efb40-9249-4d24-ad42-653b0f9b3505",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a259d3d1-8cef-4092-bbaa-e1732194c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_img(arr, d) -> list:\n",
    "    \n",
    "    \"\"\"\n",
    "    Tile the image into equal parts of size (d x d) pixels\n",
    "\n",
    "    img: PIL Image\n",
    "    d  : Side length of square tile\n",
    "\n",
    "    Return: List of PIL images\n",
    "    \"\"\"\n",
    "    img = Image.fromarray(arr)\n",
    "    w, h = img.size\n",
    "    grid = product(range(0, h-h%d, d), range(0, w-w%d, d))\n",
    "    img_list = []\n",
    "    for i, j in grid:\n",
    "        box = (j, i, j+d, i+d)\n",
    "        img_list.append(img.crop(box))\n",
    "    return img_list\n",
    "\n",
    "\n",
    "def DM_2_array(img) -> np.array:\n",
    "    \"\"\"\n",
    "    Convert Digital Micrograph file to numpy array\n",
    "\n",
    "    img: An instance of the DM3 class from _dm3_lib.py\n",
    "\n",
    "    returns a numpy array of the grayscale image\n",
    "    \"\"\"\n",
    "    nm_per_px = img.pxsize[0]\n",
    "    cons = img.contrastlimits\n",
    "    im   = img.imagedata\n",
    "    im[im>cons[1]] = cons[1]\n",
    "    im[im<cons[0]] = cons[0]\n",
    "    im =  ((im-cons[0])/(cons[1]-cons[0]))*255  #0 to 1\n",
    "    return im.astype(np.uint8), nm_per_px\n",
    "\n",
    "def load_data() -> list:\n",
    "    \n",
    "    \"\"\"\n",
    "    Opens dialogbox that allows selection of files\n",
    "    Returns file/files\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "    files = filedialog.askopenfilenames(parent=root, title='Choose a file')\n",
    "    return files\n",
    "\n",
    "\n",
    "class Prediction():\n",
    "    def __init__(self,cross):\n",
    "        self.cross     = cross\n",
    "        self.size      = 1024\n",
    "        self.tile_size = 512\n",
    "        if self.cross:\n",
    "            self.PATH = r'C:\\Users\\krist\\Documents\\masterRepo\\data\\models\\cross_unet.pth'\n",
    "        else:\n",
    "            self.PATH = r\".\\models\\length_unet.pth\"\n",
    "\n",
    "        self.checkpoint = torch.load(self.PATH, map_location=torch.device('cpu'))\n",
    "        self.model      = UNet(in_channels = 1, n_classes = 2, depth = 3, wf = 6, padding = True)\n",
    "        self.model.load_state_dict(self.checkpoint['model_state_dict'])\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        print('Model Loaded')\n",
    "    \n",
    "    def to_tensor(self, file) -> list:\n",
    "        \"\"\"\n",
    "        \n",
    "        Opens the image in grayscale, resizes it (if applicable), and converts it to a pytorch tensor\n",
    "    \n",
    "        file   (str)    : Path to file\n",
    "        Returns tensor or list of tensors\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            if file.endswith('.dm3'):\n",
    "                \n",
    "                image, nm_per_px = DM_2_array(dm.DM3(file))\n",
    "                if len(self.images) == 0:\n",
    "                    self.nm_per_px = nm_per_px\n",
    "            else:\n",
    "                image = np.array(Image.open(file).convert('L'))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Something went wrong when loading the image.\")\n",
    "            pass\n",
    "            \n",
    "        image   = cv2.resize(image, dsize = (self.size,self.size))\n",
    "        tensors = []\n",
    "        images  = tile_img(np.array(image), self.tile_size)\n",
    "\n",
    "        for i in images:\n",
    "            \n",
    "            im = np.expand_dims(i, axis=0)\n",
    "            \n",
    "            ## Can add filter if images are very noisy (Median recommended, gaussian makes the images too blurry)\n",
    "            # image = nd.median_filter(image, size=3) \n",
    "            im = 2*(im/np.max(im)) - 1\n",
    "            im = torch.tensor(im, dtype = torch.float32)\n",
    "            tensors.append(im)\n",
    "        return np.array(image), tensors\n",
    "\n",
    "\n",
    "    \n",
    "    def watershed(self,img, plot = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Performs the watershed algorithm on the prediction img\n",
    "    \n",
    "        img  : PIL.Image (semantic segmentation prediction map)\n",
    "        plot : bool (True if you want to see the watershed processing steps)\n",
    "        \n",
    "        Documentation: https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html\n",
    "        \n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = clear_border(gray)\n",
    "        ret, bin_img = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU) \n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        sure_bg = cv2.dilate(bin_img, kernel, iterations=20) \n",
    "        dist = cv2.distanceTransform(bin_img, cv2.DIST_L2, 5) \n",
    "        \n",
    "       \n",
    "        #foreground area \n",
    "        ret, sure_fg = cv2.threshold(dist, 0.15 * dist.max(), 255, cv2.THRESH_BINARY) \n",
    "        sure_fg = sure_fg.astype(np.uint8)   \n",
    "          \n",
    "        # unknown area \n",
    "        unknown = cv2.subtract(sure_bg, sure_fg) \n",
    "        ret, markers = cv2.connectedComponents(sure_fg) \n",
    "          \n",
    "        # Add one to all labels so that background is not 0, but 1 \n",
    "        markers += 1\n",
    "        markers[unknown == 255] = 0\n",
    "        markers = cv2.watershed(img, markers) \n",
    "        \n",
    "        if plot:\n",
    "            fig, axes = plt.subplots(2,2)\n",
    "            axes[0,0].imshow(gray) \n",
    "            axes[0, 0].set_title('Img') \n",
    "            axes[0,1].imshow(dist) \n",
    "            axes[0, 1].set_title('Distance Transform') \n",
    "              \n",
    "            axes[1,0].imshow(sure_fg) \n",
    "            axes[1, 0].set_title('Sure Foreground') \n",
    "            axes[1,1].imshow(markers) \n",
    "            axes[1, 1].set_title('Markers') \n",
    "    \n",
    "        img2 = color.label2rgb(markers,bg_label = 1,bg_color=(0, 0, 0))\n",
    "        props = measure.regionprops_table(markers, intensity_image=gray, \n",
    "                                      properties=['label',\n",
    "                                                  'area', 'equivalent_diameter',\n",
    "                                                  'mean_intensity', 'solidity'])\n",
    "        \n",
    "        df = pd.DataFrame(props)\n",
    "        area = list(df[(df.mean_intensity > 100) & (df.area > 1.5/self.nm_per_px**2)].area)\n",
    "        return area\n",
    "\n",
    "\n",
    "    \n",
    "    def calc_length(self, img):\n",
    "        \"\"\"\n",
    "        Estimates the length of precipitates\n",
    "        \"\"\"\n",
    "        grey = img[:,:,0]\n",
    "        contours, hierarchy = cv2.findContours(grey, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "        l      = []\n",
    "        for contour in contours:\n",
    "            (center), (width,height), angle = cv2.minAreaRect(contour)\n",
    "            length = np.max([width,height])\n",
    "            l.append([length, angle+(angle<0)*90])\n",
    "        for index, (length, angle) in enumerate(l):\n",
    "            median_angle = np.median([angle for (length, angle) in l if length*self.nm_per_px > 5]) #Find angles of all detections longer than 5nm\n",
    "            error        = 5.0 #degrees\n",
    "            if  (median_angle - error<angle<median_angle + error) and length*self.nm_per_px>3: #If precipitate in correct direction (within error) and longer than 3nm, accept detection\n",
    "                self.lengths.append(length) \n",
    "    def evaluate(self, nm_per_px):\n",
    "        \"\"\"\n",
    "        Evaluation/prediction function\n",
    "\n",
    "        nm_per_px: float (Image calibration AS IF IMAGE IS 2048x2048)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data       = load_data()\n",
    "        self.nm_per_px  = nm_per_px\n",
    "        self.prediction = []\n",
    "        self.images     = []\n",
    "        self.area       = []\n",
    "        self.lengths    = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for img in iter(self.data):\n",
    "            true_img, imgs = self.to_tensor(img)\n",
    "            new_im = Image.new('RGB', (self.size, self.size))\n",
    "            if len(self.images) == 0:\n",
    "                self.nm_per_px *=2 #Original calibration for 2048x2048, but images are resized to 1024x1024\n",
    "            for index, im in enumerate(imgs):\n",
    "                im = im.unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    pred   = self.model(im)\n",
    "                    output = torch.argmax(pred, dim=1)  # Get the index of the channel with the highest probability\n",
    "                    output = output.squeeze(0).cpu().numpy()\n",
    "                    \n",
    "                y_offset =  int(self.tile_size*(index>1))\n",
    "                x_offset =  int(self.tile_size*((index)%2))\n",
    "                out      = Image.fromarray(output.astype('uint8')*255).convert('RGB')\n",
    "                new_im.paste(out, box = (x_offset,y_offset))\n",
    "                \n",
    "            self.images.append(np.array(true_img))\n",
    "            self.prediction.append(np.array(new_im))\n",
    "\n",
    "            if self.cross:\n",
    "                self.area += self.watershed(self.prediction[-1], plot = False)\n",
    "            else:\n",
    "                self.calc_length(self.prediction[-1])\n",
    "                \n",
    "        \n",
    "        total_time = time.time()-start_time\n",
    "        print(f\"Total interference time: {np.round(total_time,2)}s ; Time per Image: {np.round(total_time/len(self.images),3)}s\")\n",
    "\n",
    "        if self.cross:\n",
    "            return np.array(self.area)*self.nm_per_px**2, self.images, self.prediction\n",
    "        else:\n",
    "            return np.array(self.lengths)*self.nm_per_px, self.images, self.prediction        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063cb66-dd29-42f7-b105-d6771acb88ae",
   "metadata": {},
   "source": [
    "## Cross-section\n",
    "\n",
    "Note: nm_per_px (Calibration) should be the calibration for a 2048x2048 image, if the images are .dm3, the manual calibration is not needed. \n",
    "\n",
    "The original images as prediction masks are found in (images, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3509356d-d7a5-4e16-9e78-f85c1f074003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\AppData\\Local\\Temp\\ipykernel_13884\\2386669199.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(self.PATH, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Total interference time: 33.06s ; Time per Image: 33.06s\n",
      "1024\n",
      "Average: 8.44nm, STDev: 4.85nm, Number counted: 30\n"
     ]
    }
   ],
   "source": [
    "cross_sections = Prediction(cross = True)\n",
    "area, images, prediction = cross_sections.evaluate(nm_per_px = 0.069661)\n",
    "print(len(prediction))\n",
    "\n",
    "print('Average: {0:.2f}nm, STDev: {1:.2f}nm, Number counted: {2:d}'.format(np.mean(area), np.std(area), len(area)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c611f3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 1024, 1024) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:3562\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   3541\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   3542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   3543\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3560\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3561\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[1;32m-> 3562\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[0;32m   3563\u001b[0m         X,\n\u001b[0;32m   3564\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[0;32m   3565\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   3566\u001b[0m         aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[0;32m   3567\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   3568\u001b[0m         alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[0;32m   3569\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[0;32m   3570\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[0;32m   3571\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m   3572\u001b[0m         extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[0;32m   3573\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   3574\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   3575\u001b[0m         filterrad\u001b[38;5;241m=\u001b[39mfilterrad,\n\u001b[0;32m   3576\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   3577\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   3578\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3579\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3580\u001b[0m     )\n\u001b[0;32m   3581\u001b[0m     sci(__ret)\n\u001b[0;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[0;32m   1474\u001b[0m             ax,\n\u001b[0;32m   1475\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args),\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: sanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:697\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (1, 1024, 1024) for image data"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0de441-4563-40a8-98b5-24f8777cc073",
   "metadata": {},
   "source": [
    "## Length\n",
    "\n",
    "Note: nm_per_px (Calibration) should be the calibration for a 2048x2048 image, if the images are .dm3, the manual calibration is not needed. \n",
    "\n",
    "The original images as prediction masks are found in (images, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bcb2bdb-38aa-43a3-9892-61cb14fc2787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\AppData\\Local\\Temp\\ipykernel_22492\\4070718504.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(self.PATH, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Total interference time: 25.26s ; Time per Image: 25.261s\n",
      "Average: 7.01nm, STDev: 1.41nm, Number counted: 3\n"
     ]
    }
   ],
   "source": [
    "length = Prediction(cross = False)\n",
    "length, images, prediction = length.evaluate(nm_per_px = 0.16685)\n",
    "\n",
    "print('Average: {0:.2f}nm, STDev: {1:.2f}nm, Number counted: {2:d}'.format(np.mean(length), np.std(length), len(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88394e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
