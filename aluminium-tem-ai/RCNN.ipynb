{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a6165f-dc4f-45fc-9ea3-e1d9fca3e64b",
   "metadata": {},
   "source": [
    "# Automatic segmentation of precipitate statistics using Mask RCNN architecture\n",
    "\n",
    "This code utilizes a Mask R-CNN deep learning architecture to automatically extract precipitate statistics from transmission electron microscopy (TEM) images. \n",
    "The code is developed as part of a master's thesis in applied physics, the code segments precipitates within the images, enabling the automatic measurement of precipitate length and cross-sections. \n",
    "By automating this process, it significantly accelerates the analysis of precipitate distributions, aiding in materials research and development.\n",
    "\n",
    "## Author:\n",
    "\n",
    "**Espen J. Gregory** - Developed for Master thesis in Physics 2024\n",
    "\n",
    "## Note:\n",
    "\n",
    "- It is recommended to have a GPU and the CUDA-version of Pytorch installed (However it is not required).\n",
    "- Make sure model files (.pth) are placed in the same folder as the notebook\n",
    "- Data can be loaded in two ways, either by directly uploading the .DM3 file, or converting the .DM3 to an image (.jpeg/.png) and manually selecting the calibration unit *nm_per_px*.\n",
    "- Mask R-CNN documentation: https://arxiv.org/abs/1703.06870"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effdb76-bd47-4b51-9b79-b7e3fdc6a06d",
   "metadata": {},
   "source": [
    "### Imports/Dependencies and Pytorch initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4313ae-2b43-4c52-b858-ff8be91af44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt5\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import _dm3_lib as dm\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from tkinter import filedialog\n",
    "from matplotlib.widgets import Button, Slider\n",
    "from skimage.segmentation import clear_border\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"PyTorch Initialization\"\"\"\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = True\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device type: %s\"%(device))\n",
    "\n",
    "\n",
    "font = {'size'   : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b358e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask-RCNN Model Loaded\n",
      "Evaluating image 1/6: C:\\Users\\krist\\Documents\\masterRepo\\data\\test_cross\\01-dm310h_jpg.rf.72eebbad25b572b6ff1245627e55c004.jpg\n",
      "Evaluating image 2/6: C:\\Users\\krist\\Documents\\masterRepo\\data\\test_cross\\012_jpg.rf.9614581b7c0a0232b0ec72d332a49471.jpg\n",
      "Evaluating image 3/6: C:\\Users\\krist\\Documents\\masterRepo\\data\\test_cross\\06-dm310h_jpg.rf.328259131328f76f78cf56bedf95173b.jpg\n",
      "Evaluating image 4/6: C:\\Users\\krist\\Documents\\masterRepo\\data\\test_cross\\1-dm316h_jpg.rf.0d5872cc83a194e1e4214845e7aa92e9.jpg\n",
      "Evaluating image 5/6: C:\\Users\\krist\\Documents\\masterRepo\\data\\test_cross\\2_jpg.rf.f09d9baa85d7b92179ecf126e40b7d72.jpg\n",
      "Evaluating image 6/6: C:\\Users\\krist\\Documents\\masterRepo\\data\\test_cross\\3-dm32h_jpg.rf.ce5a5c139142e9358a28858a8e6561fe.jpg\n"
     ]
    }
   ],
   "source": [
    "# Using DataSetEvaluator\n",
    "from testMaster.DatasetEvaluator import DatasetEvaluator, RCNNEvaluator\n",
    "\n",
    "# Example usage:\n",
    "image_dir = r'C:\\Users\\krist\\Documents\\masterRepo\\data\\test_cross'  \n",
    "\n",
    "# Initialize the RCNN evaluator (set cross=True for cross-section evaluation)\n",
    "rcnn_evaluator = RCNNEvaluator(path_model=Path(r'C:\\Users\\krist\\Documents\\masterRepo\\data\\models\\cross_rcnn.pth'), cross=True, device='cpu')\n",
    "\n",
    "# Initialize DatasetEvaluator with the path to the image folder\n",
    "dataset_evaluator = DatasetEvaluator(image_dir)\n",
    "\n",
    "# Run the predictions and generate plots for each image\n",
    "dataset_evaluator.predict(rcnn_evaluator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844296a-c466-49bb-9e2b-d4c104c13383",
   "metadata": {},
   "source": [
    "### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97624f7a-b524-4aed-b2bd-48e970cdf481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DM_2_array(img) -> np.array:\n",
    "    \"\"\"\n",
    "    Convert Digital Micrograph file to numpy array\n",
    "\n",
    "    img: An instance of the DM3 class from _dm3_lib.py\n",
    "\n",
    "    returns a numpy array of the grayscale image\n",
    "    \"\"\"\n",
    "    nm_per_px = img.pxsize[0]\n",
    "    cons = img.contrastlimits\n",
    "    im   = img.imagedata\n",
    "    im[im>cons[1]] = cons[1]\n",
    "    im[im<cons[0]] = cons[0]\n",
    "    im =  ((im-cons[0])/(cons[1]-cons[0]))*255  #0 to 1\n",
    "    return im.astype(np.uint8), nm_per_px\n",
    "    \n",
    "def load_data() -> list:\n",
    "    \"\"\"\n",
    "    Opens dialogbox that allows selection of files\n",
    "    Returns file/files\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "    files = filedialog.askopenfilenames(parent=root, title='Choose a file')\n",
    "    return files\n",
    "\n",
    "\n",
    "def check_image(n: int, thresh: float, erode: int) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "    Function used to check the mask overlay of image n, given a masking threshold thresh\n",
    "    \n",
    "    Returns: Image, Image with predicted mask overlay \n",
    "    \"\"\"\n",
    "    print(f\"Image checked: {model.data[n]}\")\n",
    "    pred = model.prediction[n]\n",
    "    im   = model.images[n]\n",
    "    gray    = cv2.cvtColor(im[0],cv2.COLOR_BGR2RGB)\n",
    "    overlay = gray.copy()\n",
    "    scr_thres = 0.9 #Confidence score threshold for the RPN (Region proposal network) \n",
    "    for i in range(len(pred[0]['masks'])):\n",
    "        msk=pred[0]['masks'][i,0].detach().cpu().numpy()\n",
    "        scr=pred[0]['scores'][i].detach().cpu().numpy()\n",
    "        box = [int(i) for i in pred[0]['boxes'][i].detach().cpu().numpy()]\n",
    "        if scr>scr_thres:\n",
    "\n",
    "            mask    = msk>thresh\n",
    "            kernel  = np.ones((2, 2), np.uint8) \n",
    "            mask_er = cv2.erode(mask.astype(np.float32), kernel, iterations = erode)  \n",
    "            mask    = mask_er>0\n",
    "            overlay[:,:,:][mask] =  [1,0,0] #Makes mask overlay red\n",
    "    im   = 0\n",
    "    pred = 0\n",
    "    return gray, overlay\n",
    "\n",
    "\n",
    "def update(erode, val):\n",
    "    \"\"\"\n",
    "    Function that updates the matplotlib figure when the slider is moved.\n",
    "    \"\"\"\n",
    "    global erode_it_temp, temp_threshold\n",
    "    if erode:\n",
    "        erode_it_temp = round(val)\n",
    "    else:\n",
    "        temp_threshold = val\n",
    "    gray, overlay = check_image(n, temp_threshold ,erode_it_temp)\n",
    "    ax[1].imshow(overlay)\n",
    "    fig.canvas.draw_idle()\n",
    "def Check_Mask(n, model):\n",
    "    \n",
    "    global button, thresh_slider,thresh_slider2, threshold, temp_threshold, fig, ax, erode_it, erode_it_temp\n",
    "    temp_threshold = model.threshold\n",
    "    erode_it_temp = model.erode_it\n",
    "\n",
    "    accept = False\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,10))\n",
    "    gray, overlay = check_image(n, model.threshold ,model.erode_it)\n",
    "    ax[0].imshow(gray)\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(overlay)\n",
    "    ax[1].axis('off')\n",
    "    axthresh  = fig.add_axes([0.125, 0.06, 0.775, 0.03])\n",
    "    axthresh2 = fig.add_axes([0.125, 0.1, 0.775, 0.03])\n",
    "    thresh_slider2 = Slider(ax=axthresh2, label='Mask threshold', valmin=0, valmax=1,valinit=model.threshold)\n",
    "    thresh_slider  = Slider(ax=axthresh, label='Erode iterations', valmin=0, valmax=10,valinit=model.erode_it,valfmt=\"%i\")\n",
    "    def accept(event):\n",
    "        model.threshold = temp_threshold\n",
    "        model.erode_it  = erode_it_temp\n",
    "        plt.close()\n",
    "    thresh_slider.on_changed(lambda x: update(True,x))\n",
    "    thresh_slider2.on_changed(lambda x: update(False,x))\n",
    "    resetax = fig.add_axes([0.8, 0.01, 0.1, 0.04])\n",
    "    button = Button(resetax, 'Accept', hovercolor='0.975')\n",
    "    button.on_clicked(accept)\n",
    "\n",
    "\n",
    "class Prediction():\n",
    "    def __init__(self,cross):\n",
    "        self.cross     = cross\n",
    "        self.size      = 1024\n",
    "        \n",
    "        \n",
    "        if self.cross:\n",
    "            self.threshold = 0.9\n",
    "            self.erode_it  = 0\n",
    "            self.PATH = r\"C:\\Users\\krist\\Documents\\masterRepo\\data\\models\\cross_rcnn.pth\"\n",
    "        else:\n",
    "            self.erode_it  = 4\n",
    "            self.threshold = 0.5\n",
    "            self.PATH = r\".\\models\\length_rcnn.pth\"\n",
    "\n",
    "        self.checkpoint = torch.load(self.PATH, map_location=torch.device('cpu'))\n",
    "        self.model      = tv.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT', min_size=1024, max_size=2048, box_detections_per_img = 500) \n",
    "        in_features     = self.model.roi_heads.box_predictor.cls_score.in_features \n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)\n",
    "        self.model.load_state_dict(self.checkpoint['model_state_dict'])\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        print('Model Loaded')\n",
    "    \n",
    "    def to_tensor(self, file) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Opens the image in grayscale, resizes it (if applicable), and converts it to a pytorch tensor\n",
    "    \n",
    "        file   (str)    : Path to file\n",
    "            \n",
    "        Returns tensor\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if file.endswith('.dm3'):\n",
    "                image, nm_per_px = DM_2_array(dm.DM3(file))\n",
    "                if len(self.images) == 0:\n",
    "                    self.nm_per_px = nm_per_px\n",
    "            else:\n",
    "                image = np.array(Image.open(file).convert('L'))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Something went wrong when loading the image.\")\n",
    "            pass\n",
    "        image = cv2.resize(image, dsize = (self.size,self.size))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        ## Can add filter if images are very noisy (Median recommended, gaussian makes the images too blurry)\n",
    "        # image = nd.median_filter(image, size=3) \n",
    "        image = image/np.max(image)\n",
    "        image = torch.tensor(image, dtype = torch.float32)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def evaluate(self, nm_per_px):\n",
    "        \"\"\"\n",
    "        Prediction Mask R-CNN\n",
    "\n",
    "        nm_per_px: float (Image calibration AS IF IMAGE IS 2048x2048)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data       = load_data()\n",
    "        self.nm_per_px  = nm_per_px\n",
    "        self.prediction = []\n",
    "        self.images     = []\n",
    "        self.lengths    = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for img in iter(self.data):\n",
    "            im = self.to_tensor(img).unsqueeze(0).to(device)\n",
    "            if len(self.images) == 0:\n",
    "                self.nm_per_px *=2 #Original calibration for 2048x2048, but images are resized to 1024x1024\n",
    "            with torch.no_grad(): #Predict\n",
    "                pred = self.model(im)\n",
    "                self.prediction.append(pred)\n",
    "            im = im[0].detach().cpu().numpy()\n",
    "            self.images.append(im)\n",
    "        total_time = time.time()-start_time\n",
    "        print(f\"Total interference time: {np.round(total_time,2)}s ; Time per Image: {np.round(total_time/len(self.images),3)}s\")\n",
    "    def statistics(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.area    = []\n",
    "        self.lengths = []\n",
    "        print('Mask threshold set to {0:.2f}'.format(self.threshold))\n",
    "        print('Calibration used: {0:.4f} nm/px'.format(self.nm_per_px))\n",
    "        \n",
    "        if self.cross:\n",
    "            for pred in self.prediction:\n",
    "                for i in range(len(pred[0]['masks'])):\n",
    "                    box = pred[0]['boxes'][i].detach().cpu().numpy()\n",
    "                    msk = pred[0]['masks'][i,0].detach().cpu().numpy()\n",
    "                    scr = pred[0]['scores'][i].detach().cpu().numpy()\n",
    "                    mask    = msk>self.threshold\n",
    "                    kernel  = np.ones((2, 2), np.uint8) \n",
    "                    mask_er = cv2.erode(mask.astype(np.float32), kernel, iterations = self.erode_it)  \n",
    "                    msk    = mask_er>0\n",
    "                    clear = clear_border(msk)\n",
    "                    area1 = np.sum(clear)\n",
    "                    area2 = np.sum(msk)\n",
    "                    if scr>0.9 and area1 == area2:\n",
    "                        self.area.append(area1)\n",
    "            return np.array(self.area)*self.nm_per_px**2\n",
    "        else:\n",
    "            for pred in self.prediction:\n",
    "                for i in range(len(pred[0]['masks'])):\n",
    "                    scr = pred[0]['scores'][i].detach().cpu().numpy()\n",
    "                    box = pred[0]['boxes'][i].detach().cpu().numpy()\n",
    "                    msk = pred[0]['masks'][i,0].detach().cpu().numpy()\n",
    "                    mask    = msk>self.threshold\n",
    "                    kernel  = np.ones((2, 2), np.uint8) \n",
    "                    mask_er = cv2.erode(mask.astype(np.float32), kernel, iterations = self.erode_it)  \n",
    "                    msk     = clear_border(mask_er>0,buffer_size=10)\n",
    "                    if scr>0.9 and np.any(msk):\n",
    "                        rect = cv2.minAreaRect(np.argwhere((msk>self.threshold)))\n",
    "                        (center), (width,height), angle = rect\n",
    "                        length = np.max([width,height])\n",
    "                        self.lengths.append(length)\n",
    "            return np.array(self.lengths)*self.nm_per_px\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9a010-e746-4a12-b97b-d3aa0ff5e933",
   "metadata": {},
   "source": [
    "# Cross-section\n",
    "\n",
    "**Note:** \n",
    "- nm_per_px (Calibration) should be the calibration for a 2048x2048 image, if the images are .dm3, the manual calibration is not needed.\n",
    "- If program runs slow, restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7333c652-c0c8-4490-bd32-ea2db1f8adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\AppData\\Local\\Temp\\ipykernel_2220\\3299790051.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(self.PATH, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Total interference time: 20.89s ; Time per Image: 20.89s\n"
     ]
    }
   ],
   "source": [
    "model = Prediction(cross = True)\n",
    "model.evaluate(nm_per_px = 0.069661) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c534da7-aca3-44f5-83a7-43e822721a0f",
   "metadata": {},
   "source": [
    "#### Use Check_Mask to adjust threshold, and erosion iterations\n",
    "\n",
    "**Note**\n",
    "- Default values should be good\n",
    "- Lowering erosion and mask threshold makes mask bigger\n",
    "- n : Index of image you want to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6ddc78-b1dc-4350-bb1d-e15d1ccdeb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n",
      "Image checked: C:/Users/krist/Documents/masterRepo/data/train_cross/3-dm32h_jpg.rf.3359921eb081c5b47c063d42cdfd8363.jpg\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "\n",
    "Check_Mask(n, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7311a78-c625-4a71-abaf-665fed0c90c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask threshold set to 0.90\n",
      "Calibration used: 0.1393 nm/px\n",
      "Average: 10.21nm, STDev: 3.45nm, Number counted: 28\n"
     ]
    }
   ],
   "source": [
    "area = model.statistics()\n",
    "\n",
    "print('Average: {0:.2f}nm, STDev: {1:.2f}nm, Number counted: {2:d}'.format(np.mean(area), np.std(area), len(area)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a29a4f-bb31-4150-9c1d-e9be8e71e4e5",
   "metadata": {},
   "source": [
    "**Clear memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a94aa1e-0a14-4c37-961c-800749e25f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7de4c-a814-4bea-8fab-ed2ecee3736a",
   "metadata": {},
   "source": [
    "# Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33c6b3e-8157-456f-8862-ee0c7902eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\AppData\\Local\\Temp\\ipykernel_20888\\3463015145.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(self.PATH, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Total interference time: 14.83s ; Time per Image: 14.826s\n"
     ]
    }
   ],
   "source": [
    "model = Prediction(cross = False)\n",
    "model.evaluate(nm_per_px = 0.20835) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b389a605-4597-430a-80d8-6d9f6f44fb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image checked: C:/Users/krist/OneDrive/Dokumenter/masterProsjekt/training/training_data/valid/3-dm32h_jpg.rf.ce5a5c139142e9358a28858a8e6561fe.jpg\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "Check_Mask(n,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341aeb21-2001-48f3-899f-565d951ff7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask threshold set to 0.50\n",
      "Calibration used: 0.4167 nm/px\n",
      "Average: 12.67nm, STDev: 3.20nm, Number counted: 3\n"
     ]
    }
   ],
   "source": [
    "l = model.statistics()\n",
    "print('Average: {0:.2f}nm, STDev: {1:.2f}nm, Number counted: {2:d}'.format(np.mean(l), np.std(l), len(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87629f2-8ab6-4020-a86f-f2dbf5ac32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
